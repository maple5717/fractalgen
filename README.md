# Fractal Generative Models

[![arXiv](https://img.shields.io/badge/arXiv%20paper-2502.17437-b31b1b.svg)](https://arxiv.org/abs/2502.17437)&nbsp;
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/LTH14/fractalgen/blob/main/demo/run_fractalgen.ipynb)

<p align="center">
  <img src="demo/visual.gif" width="100%">
</p>

This is a PyTorch/GPU implementation of the paper [Fractal Generative Models](https://arxiv.org/abs/2502.17437):

```
@article{li2025fractal,
  title={Fractal Generative Models},
  author={Li, Tianhong and Sun, Qinyi and Fan, Lijie and He, Kaiming},
  journal={arXiv preprint arXiv:2502.17437},
  year={2025}
}
```

FractalGen enables pixel-by-pixel high-resolution image generation for the first time. This repo contains:

* ü™ê A simple PyTorch implementation of [Fractal Generative Model](models/fractalgen.py).
* ‚ö°Ô∏è Pre-trained pixel-by-pixel generation models trained on ImageNet 64x64 and 256x256.
* üí• A self-contained [Colab notebook](http://colab.research.google.com/github/LTH14/fractalgen/blob/main/demo/run_fractalgen.ipynb) for running pre-trained models tasks.
* üõ∏ A [training and evaluation script](main_fractalgen.py) using PyTorch DDP.

## Preparation

### Dataset
Download [ImageNet](http://image-net.org/download) dataset, and place it in your `IMAGENET_PATH`.

### Installation

Download the code:
```
git clone https://github.com/LTH14/fractalgen.git
cd fractalgen
```

A suitable [conda](https://conda.io/) environment named `fractalgen` can be created and activated with:

```
conda env create -f environment.yaml
conda activate fractalgen
```

Download pre-trained models:

```
python util/download.py
```

For convenience, our pre-trained models can be downloaded directly here as well:

| Model                                                                                                                                                 | FID-50K  | Inception Score | #params   | 
|-------------------------------------------------------------------------------------------------------------------------------------------------------|----------|-----------------|-----------|
| [FractalAR (IN64)](https://www.dropbox.com/scl/fi/n25tbij7aqkwo1ypqhz72/checkpoint-last.pth?rlkey=2czevgex3ocg2ae8zde3xpb3f&st=mj0subup&dl=0)         | 5.30     | 56.8            | 432M      |
| [FractalMAR (IN64)](https://www.dropbox.com/scl/fi/lh7fmv48pusujd6m4kcdn/checkpoint-last.pth?rlkey=huihey61ok32h28o3tbbq6ek9&st=fxtoawba&dl=0)        | 2.72     | 87.9            | 432M      |
| [FractalMAR-Base (IN256)](https://www.dropbox.com/scl/fi/zrdm7853ih4tcv98wmzhe/checkpoint-last.pth?rlkey=htq9yuzovet7d6ioa64s1xxd0&st=4c4d93vs&dl=0)  | 11.80    | 274.3           | 186M      |
| [FractalMAR-Large (IN256)](https://www.dropbox.com/scl/fi/y1k05xx7ry8521ckxkqgt/checkpoint-last.pth?rlkey=wolq4krdq7z7eyjnaw5ndhq6k&st=vjeu5uzo&dl=0) | 7.30     | 334.9           | 438M      |
| [FractalMAR-Huge (IN256)](https://www.dropbox.com/scl/fi/t2rru8xr6wm23yvxskpww/checkpoint-last.pth?rlkey=dn9ss9zw4zsnckf6bat9hss6h&st=y7w921zo&dl=0)  | 6.15     | 348.9           | 848M      |

## Usage

### Demo
Run our interactive visualization [demo](http://colab.research.google.com/github/LTH14/fractalgen/blob/main/demo/run_fractalgen.ipynb) using Colab notebook!

### Training-CIFAR10
Example script for training FractalAR
```
# --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT} \
CUDA_VISIBLE_DEVICES=3,5 torchrun --nproc_per_node=2 --nnodes=1 --node_rank=0 main_fractalgen_cifar.py 
--model fractalar_cifar32 
--img_size 64 
--class_num 10
--num_conds 1 
--batch_size 64 
--eval_freq 5 
--save_last_freq 10 
--epochs 100 
--warmup_epochs 5 
--blr 5.0e-5 
--weight_decay 0.05 
--attn_dropout 0.1 --proj_dropout 0.1 
--lr_schedule cosine 
--gen_bsz 256 --num_images 8000 
--num_iter_list 64,16 
--cfg 11.0 
--cfg_schedule linear 
--temperature 1.03 
--output_dir ./out 
--resume ./out  
--grad_checkpointing --online_eval

```

### Training-ImageNet
The below training scripts have been tested on 4x8 H100 GPUs.

Example script for training FractalAR on ImageNet 64x64 for 800 epochs:
```
torchrun --nproc_per_node=8 --nnodes=4 --node_rank=${NODE_RANK} --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT} \
main_fractalgen.py \
--model fractalar_in64 --img_size 64 --num_conds 1 \
--batch_size 64 --eval_freq 40 --save_last_freq 10 \
--epochs 800 --warmup_epochs 40 \
--blr 5.0e-5 --weight_decay 0.05 --attn_dropout 0.1 --proj_dropout 0.1 --lr_schedule cosine \
--gen_bsz 256 --num_images 8000 --num_iter_list 64,16 --cfg 11.0 --cfg_schedule linear --temperature 1.03 \
--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \
--data_path ${IMAGENET_PATH} --grad_checkpointing --online_eval
```

Example script for training FractalMAR on ImageNet 64x64 for 800 epochs:
```
torchrun --nproc_per_node=8 --nnodes=4 --node_rank=${NODE_RANK} --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT} \
main_fractalgen.py \
--model fractalmar_in64 --img_size 64 --num_conds 5 \
--batch_size 64 --eval_freq 40 --save_last_freq 10 \
--epochs 800 --warmup_epochs 40 \
--blr 5.0e-5 --weight_decay 0.05 --attn_dropout 0.1 --proj_dropout 0.1 --lr_schedule cosine \
--gen_bsz 256 --num_images 8000 --num_iter_list 64,16 --cfg 6.5 --cfg_schedule linear --temperature 1.02 \
--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \
--data_path ${IMAGENET_PATH} --grad_checkpointing --online_eval
```

Example script for training FractalMAR-L on ImageNet 256x256 for 800 epochs:
```
torchrun --nproc_per_node=8 --nnodes=4 --node_rank=${NODE_RANK} --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT} \
main_fractalgen.py \
--model fractalmar_large_in256 --img_size 256 --num_conds 5 --guiding_pixel \
--batch_size 32 --eval_freq 40 --save_last_freq 10 \
--epochs 800 --warmup_epochs 40 \
--blr 5.0e-5 --weight_decay 0.05 --attn_dropout 0.1 --proj_dropout 0.1 --lr_schedule cosine \
--gen_bsz 256 --num_images 8000 --num_iter_list 64,16,16 --cfg 21.0 --cfg_schedule linear --temperature 1.1 \
--output_dir ${OUTPUT_DIR} --resume ${OUTPUT_DIR} \
--data_path ${IMAGENET_PATH} --grad_checkpointing --online_eval
```

### Evaluation

Evaluate pre-trained FractalAR on ImageNet 64x64 unconditional likelihood estimation (single GPU):
```
torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 \
main_fractalgen.py \
--model fractalar_in64 --img_size 64 --num_conds 1 \
--nll_bsz 128 --nll_forward_number 1 \
--output_dir pretrained_models/fractalar_in64 \
--resume pretrained_models/fractalar_in64 \
--data_path ${IMAGENET_PATH} --seed 0 --evaluate_nll
```

Evaluate pre-trained FractalMAR on ImageNet 64x64 unconditional likelihood estimation (single GPU):
```
torchrun --nproc_per_node=1 --nnodes=1 --node_rank=0 \
main_fractalgen.py \
--model fractalmar_in64 --img_size 64 --num_conds 5 \
--nll_bsz 128 --nll_forward_number 10 \
--output_dir pretrained_models/fractalmar_in64 \
--resume pretrained_models/fractalmar_in64 \
--data_path ${IMAGENET_PATH} --seed 0 --evaluate_nll
```

Evaluate pre-trained FractalAR on ImageNet 64x64 class-conditional generation:
```
torchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \
main_fractalgen.py \
--model fractalar_in64 --img_size 64 --num_conds 1 \
--gen_bsz 512 --num_images 50000 \
--num_iter_list 64,16 --cfg 11.0 --cfg_schedule linear --temperature 1.03 \
--output_dir pretrained_models/fractalar_in64 \
--resume pretrained_models/fractalar_in64 \
--data_path ${IMAGENET_PATH} --seed 0 --evaluate_gen
```

Evaluate pre-trained FractalMAR on ImageNet 64x64 class-conditional generation:
```
torchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \
main_fractalgen.py \
--model fractalmar_in64 --img_size 64 --num_conds 5 \
--gen_bsz 1024 --num_images 50000 \
--num_iter_list 64,16 --cfg 6.5 --cfg_schedule linear --temperature 1.02 \
--output_dir pretrained_models/fractalmar_in64 \
--resume pretrained_models/fractalmar_in64 \
--data_path ${IMAGENET_PATH} --seed 0 --evaluate_gen
```

Evaluate pre-trained FractalMAR-Huge on ImageNet 256x256 class-conditional generation:
```
torchrun --nproc_per_node=8 --nnodes=1 --node_rank=0 \
main_fractalgen.py \
--model fractalmar_huge_in256 --img_size 256 --num_conds 5 --guiding_pixel \
--gen_bsz 1024 --num_images 50000 \
--num_iter_list 64,16,16 --cfg 19.0 --cfg_schedule linear --temperature 1.1 \
--output_dir pretrained_models/fractalmar_huge_in256 \
--resume pretrained_models/fractalmar_huge_in256 \
--data_path ${IMAGENET_PATH} --seed 0 --evaluate_gen
```

For ImageNet 256x256, the optimal classifier-free guidance values `--cfg` that achieve the best FID are `29.0` for FractalMAR-Base and `21.0` for FractalMAR-Large.

## Acknowledgements

We thank Google TPU Research Cloud (TRC) for granting us access to TPUs, and Google Cloud Platform for supporting GPU resources.

## Contact

If you have any questions, feel free to contact me through email (tianhong@mit.edu). Enjoy!
